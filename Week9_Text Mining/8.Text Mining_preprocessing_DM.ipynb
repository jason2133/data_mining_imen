{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d5f0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sususu\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\sususu\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\sususu\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sususu\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sususu\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sususu\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3b5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('webtext')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9894ee",
   "metadata": {},
   "source": [
    "### 토큰화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b4f27df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\", '!']\n"
     ]
    }
   ],
   "source": [
    "cop=\"Hello everyone. It's good to see you. Let's start our text mining class!!\"\n",
    "print(sent_tokenize(cop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a06ab39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요~ 텍스트마이닝 수업시간입니다.', '만나서 반갑습니다!', '!']\n"
     ]
    }
   ],
   "source": [
    "kor_cop=\"안녕하세요~ 텍스트마이닝 수업시간입니다. 만나서 반갑습니다!!\"\n",
    "print(sent_tokenize(kor_cop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "583fd50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(cop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d57b4aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(cop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20bc2a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요~', '텍스트마이닝', '수업시간입니다', '.', '만나서', '반갑습니다', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(kor_cop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe203860",
   "metadata": {},
   "source": [
    "### 정규표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaf36c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e654527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[abc]','How are you, boy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3644a4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[0123456789]','we have 3 books and 7 notes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ac8353e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[0-9]','we have 3 books and 7 notes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "348c0872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아  범위  0 100  까지는 70  밖에 안되는데        35쪽 부터 보자\n"
     ]
    }
   ],
   "source": [
    "inputString='아! 범위 [0~100] 까지는 70% 밖에 안되는데! @_@  #35쪽 부터 보자'\n",
    "res1=re.sub('[-=+,#/\\?:^.@*\\\"※~ㆍ!%_』‘|\\(\\)\\[\\]`\\'…》\\”\\“\\’·]', ' ', inputString)\n",
    "print(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a8f11eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아', '범위', '0', '100', '까지는', '70', '밖에', '안되는데', '35쪽', '부터', '보자']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff76aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "64d015c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sorry', 'I', \"can't\", 'go', 'there']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"sorry, I can't go there.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e3486c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sorry', 'I', 'can', 't', 'go', 'there']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "tokenizer.tokenize(\"sorry, I can't go there.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316a5c0",
   "metadata": {},
   "source": [
    "### 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "968db8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on', 'ma', 'such', 'they', 'be', 'few', 'out', 'm', 'will', 'o', 'do', 'couldn', \"shan't\", 'me', 'did', 'i', 'those', 'were', \"should've\", \"wasn't\", 've', 'had', 'in', 'where', 'both', 'having', 'won', \"doesn't\", 'over', 'yourself', \"aren't\", 'its', 'myself', 'll', 'again', \"shouldn't\", 'from', 'what', 'an', 'is', 'during', 'same', 'hadn', 't', 'whom', 'after', 'mightn', 'didn', 'further', \"needn't\", 'but', 'these', 'hasn', 'yourselves', 'them', \"isn't\", 'off', \"you'd\", \"don't\", 'why', 'aren', 'does', 'before', 'most', \"mightn't\", 'our', 'their', 'nor', 'only', \"didn't\", 'under', 'other', \"you've\", 'once', 'then', 'ourselves', 'there', 'he', 'at', 'd', 'have', 'your', 'some', \"haven't\", 'mustn', 'that', 'am', 'my', 'was', 'of', \"wouldn't\", \"hadn't\", 'any', 'you', 'to', 'can', 're', 'all', 'needn', 'between', 'she', \"it's\", 'now', 'him', 'a', 'or', 'up', \"hasn't\", 'wasn', 'for', 'ain', \"couldn't\", 'which', 'if', 'itself', 'this', 'more', 'her', 'hers', 'too', 'weren', 'theirs', 'each', \"weren't\", 'through', 'how', 'being', 'below', 'into', \"mustn't\", 'y', \"she's\", 'just', 'ours', \"that'll\", 'by', 'who', 'shan', 'shouldn', 'himself', 'doesn', 'while', 'should', \"won't\", 's', 'are', \"you'll\", 'no', 'and', 'has', 'very', 'down', 'his', 'when', 'wouldn', 'yours', 'we', 'the', 'been', 'don', 'herself', 'isn', 'here', 'not', 'own', 'than', 'with', 'doing', 'above', 'because', 'against', 'haven', 'themselves', 'until', \"you're\", 'as', 'it', 'about', 'so'}\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "# nltk가 제공하는 영어 stopword를 확인\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "english_stops=set(stopwords.words('english'))\n",
    "print(english_stops)\n",
    "\n",
    "print(len(english_stops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95fc18a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- copus ----------\n",
      "Sorry, I couldn't fo to movie yesterday\n",
      "---------- word_tokenize로 토큰화 ----------\n",
      "['sorry', 'i', \"couldn't\", 'fo', 'to', 'movie', 'yesterday']\n",
      "---------- 불용어 제거 결과 ----------\n",
      "['sorry', 'fo', 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "textA=\"Sorry, I couldn't fo to movie yesterday\"\n",
    "print('-'*10,'copus','-'*10)\n",
    "print(textA)\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w']+\")\n",
    "tokens=tokenizer.tokenize(textA.lower())\n",
    "print('-'*10,'word_tokenize로 토큰화','-'*10)\n",
    "print(tokens)\n",
    "\n",
    "rmv_stopwd=[word for word in tokens if word not in english_stops]\n",
    "print('-'*10,'불용어 제거 결과','-'*10)\n",
    "print(rmv_stopwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02bbbc",
   "metadata": {},
   "source": [
    "### 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffee2415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook -> cook\n",
      "cooking -> cook\n",
      "cooks -> cook\n",
      "cooker -> cooker\n",
      "cookery -> cookeri\n",
      "cookbooks -> cookbook\n"
     ]
    }
   ],
   "source": [
    "# 포터 스테머\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print('cook ->',stemmer.stem('cook'))\n",
    "print('cooking ->',stemmer.stem('cooking'))\n",
    "print('cooks ->',stemmer.stem('cooks'))\n",
    "print('cooker ->',stemmer.stem('cooker'))\n",
    "print('cookery ->',stemmer.stem('cookery'))\n",
    "print('cookbooks ->',stemmer.stem('cookbooks'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c014822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- copus ----------\n",
      "I like our text mining class! She likes our text mining classes!\n",
      "---------- word_tokenize ----------\n",
      "['I', 'like', 'our', 'text', 'mining', 'class', '!', 'She', 'likes', 'our', 'text', 'mining', 'classes', '!']\n",
      "---------- porter stemming ----------\n",
      "['i', 'like', 'our', 'text', 'mine', 'class', '!', 'she', 'like', 'our', 'text', 'mine', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "# 포터 스테머\n",
    "\n",
    "cop=\"I like our text mining class! She likes our text mining classes!\"\n",
    "print('-'*10,'copus','-'*10)\n",
    "print(cop)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens=word_tokenize(cop)\n",
    "print('-'*10,'word_tokenize','-'*10)\n",
    "print(tokens)\n",
    "\n",
    "result=[stemmer.stem(token) for token in tokens]\n",
    "print('-'*10,'porter stemming','-'*10)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "625b7a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook -> cook\n",
      "cooking -> cook\n",
      "cooks -> cook\n",
      "cooker -> cook\n",
      "cookery -> cookery\n",
      "cookbooks -> cookbook\n"
     ]
    }
   ],
   "source": [
    "# 랭카스터 스테머\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print('cook ->',stemmer.stem('cook'))\n",
    "print('cooking ->',stemmer.stem('cooking'))\n",
    "print('cooks ->',stemmer.stem('cooks'))\n",
    "print('cooker ->',stemmer.stem('cooker'))\n",
    "print('cookery ->',stemmer.stem('cookery'))\n",
    "print('cookbooks ->',stemmer.stem('cookbooks'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47848316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook -> cook\n",
      "cooking -> cooking\n",
      "cooking -> cook\n",
      "cooks -> cook\n",
      "cooker -> cooker\n",
      "cookery -> cookery\n",
      "cookbooks -> cookbook\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print('cook ->',lemmatizer.lemmatize('cook'))\n",
    "print('cooking ->',lemmatizer.lemmatize('cooking'))\n",
    "print('cooking ->',lemmatizer.lemmatize('cooking',pos='v'))\n",
    "print('cooks ->',lemmatizer.lemmatize('cooks'))\n",
    "print('cooker ->',lemmatizer.lemmatize('cooker'))\n",
    "print('cookery ->',lemmatizer.lemmatize('cookery'))\n",
    "print('cookbooks ->',lemmatizer.lemmatize('cookbooks'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdf78cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n",
      "1.0000000000000002\n",
      "0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "# 코사인 유사도 예제\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "\n",
    "doc1 = np.array([0, 1, 1, 1])\n",
    "doc2 = np.array([1, 0, 1, 1])\n",
    "doc3 = np.array([2, 0, 2, 2])\n",
    "\n",
    "print(cos_sim(doc1, doc2))\n",
    "print(cos_sim(doc2, doc3))\n",
    "print(cos_sim(doc1, doc3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
